{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdO2w6WF0brQtyUg0TN9/p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dietmarja/LLM-Elements/blob/main/QLoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0oxFjis7Zri"
      },
      "outputs": [],
      "source": [
        "# QLoRA (Quantized Low-Rank Adaptation). QLoRA typically involves quantizing the weights of the model in addition to applying a low-rank adaptation.\n",
        "# For simplicity, we'll use a basic quantization approach where we scale the weights to integers and then scale them back during computation.\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# QLoRA Layer Definition\n",
        "class QLoRALayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, rank, quant_bits=8):\n",
        "        super(QLoRALayer, self).__init__()\n",
        "        self.rank = rank\n",
        "        self.quant_bits = quant_bits\n",
        "        self.W = nn.Linear(in_features, out_features, bias=False)\n",
        "        self.A = nn.Linear(in_features, rank, bias=False)\n",
        "        self.B = nn.Linear(rank, out_features, bias=False)\n",
        "\n",
        "        # Initialize A and B with small values\n",
        "        nn.init.normal_(self.A.weight, std=0.01)\n",
        "        nn.init.normal_(self.B.weight, std=0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.quantize(self.W(x) + self.B(self.A(x)))\n",
        "\n",
        "    def quantize(self, x):\n",
        "        scale = (2 ** self.quant_bits - 1) / x.max()\n",
        "        return torch.round(x * scale) / scale\n",
        "\n",
        "    def print_weights(self):\n",
        "        print(f\"Full-rank weight matrix (W): \\n{self.W.weight.data}\")\n",
        "        print(f\"Low-rank weight matrix A: \\n{self.A.weight.data}\")\n",
        "        print(f\"Low-rank weight matrix B: \\n{self.B.weight.data}\")\n",
        "\n",
        "# Simple Model Definition\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, qlora_rank=None):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        if qlora_rank:\n",
        "            self.layer = QLoRALayer(input_dim, output_dim, qlora_rank)\n",
        "        else:\n",
        "            self.layer = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "    def print_weights(self):\n",
        "        if isinstance(self.layer, QLoRALayer):\n",
        "            self.layer.print_weights()\n",
        "        else:\n",
        "            print(f\"Full-rank weight matrix (W): \\n{self.layer.weight.data}\")\n",
        "\n",
        "# Training Function\n",
        "def train_model(model, inputs, targets, epochs=100):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "    losses = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        if (epoch+1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    return losses\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(model, test_inputs):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(test_inputs)\n",
        "        print(f'Predictions: {predictions}')\n",
        "\n",
        "# Define model parameters\n",
        "input_dim = 10\n",
        "output_dim = 5\n",
        "\n",
        "# Dummy data for demonstration\n",
        "inputs = torch.randn(8, input_dim)\n",
        "targets = torch.randn(8, output_dim)\n",
        "\n",
        "# QLoRA ranks to evaluate\n",
        "qlora_ranks = [1, 2, 3, 4]\n",
        "\n",
        "# Dictionary to store all losses\n",
        "all_losses = {\"No QLoRA\": train_model(SimpleModel(input_dim, output_dim), inputs, targets)}\n",
        "\n",
        "# Train models with different QLoRA ranks\n",
        "for rank in qlora_ranks:\n",
        "    print(f\"Training with QLoRA rank {rank}...\")\n",
        "    model_qlora = SimpleModel(input_dim, output_dim, rank)\n",
        "    losses_qlora = train_model(model_qlora, inputs, targets)\n",
        "    all_losses[f\"QLoRA rank {rank}\"] = losses_qlora\n",
        "    model_qlora.print_weights()\n",
        "\n",
        "# Plot the losses for all models\n",
        "plt.figure(figsize=(12, 8))\n",
        "for label, losses in all_losses.items():\n",
        "    plt.plot(losses, label=label)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss with and without QLoRA')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Example prediction\n",
        "test_inputs = torch.randn(2, input_dim)\n",
        "for rank in [\"No QLoRA\"] + [f\"QLoRA rank {r}\" for r in qlora_ranks]:\n",
        "    print(f\"Evaluating model with {rank}...\")\n",
        "    model = SimpleModel(input_dim, output_dim) if rank == \"No QLoRA\" else SimpleModel(input_dim, output_dim, int(rank.split()[-1]))\n",
        "    evaluate_model(model, test_inputs)\n"
      ]
    }
  ]
}