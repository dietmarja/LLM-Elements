{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAn+ACNVhn/9UW3CosvOqr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dietmarja/LLM-Elements/blob/main/QLoRA_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QLoRA (Quantized Low-Rank Adaptation).\n",
        "## QLoRA typically involves quantizing the weights of the model in addition toapplying a low-rank adaptation. For simplicity, we'll use a basic quantization approach where we scale the weights to integers and then scale them back during computation.\n"
      ],
      "metadata": {
        "id": "yh4ki5XIjx11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "G7OYJKj0jlOB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# QLoRA Layer Definition\n",
        "class QLoRALayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, rank, quant_bits=8):\n",
        "        super(QLoRALayer, self).__init__()\n",
        "        self.rank = rank\n",
        "        self.quant_bits = quant_bits\n",
        "        self.W = nn.Linear(in_features, out_features, bias=False)\n",
        "        self.A = nn.Linear(in_features, rank, bias=False)\n",
        "        self.B = nn.Linear(rank, out_features, bias=False)\n",
        "\n",
        "        # Initialize A and B with small values\n",
        "        nn.init.normal_(self.A.weight, std=0.01)\n",
        "        nn.init.normal_(self.B.weight, std=0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.quantize(self.W(x) + self.B(self.A(x)))\n",
        "\n",
        "    def quantize(self, x):\n",
        "        scale = (2 ** self.quant_bits - 1) / x.max()\n",
        "        return torch.round(x * scale) / scale\n",
        "\n",
        "    def print_weights(self):\n",
        "        print(f\"Full-rank weight matrix (W): \\n{self.W.weight.data}\")\n",
        "        print(f\"Low-rank weight matrix A: \\n{self.A.weight.data}\")\n",
        "        print(f\"Low-rank weight matrix B: \\n{self.B.weight.data}\")\n"
      ],
      "metadata": {
        "id": "rGPvgMXOjnqg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple Model Definition\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, qlora_rank=None):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        if qlora_rank:\n",
        "            self.layer = QLoRALayer(input_dim, output_dim, qlora_rank)\n",
        "        else:\n",
        "            self.layer = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "    def print_weights(self):\n",
        "        if isinstance(self.layer, QLoRALayer):\n",
        "            self.layer.print_weights()\n",
        "        else:\n",
        "            print(f\"Full-rank weight matrix (W): \\n{self.layer.weight.data}\")"
      ],
      "metadata": {
        "id": "QTb7306Tjn9h"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Function\n",
        "def train_model(model, inputs, targets, epochs=100):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "    losses = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        if (epoch+1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    return losses"
      ],
      "metadata": {
        "id": "0a8s_oKfjoLE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Function\n",
        "def evaluate_model(model, test_inputs):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(test_inputs)\n",
        "        print(f'Predictions: {predictions}')"
      ],
      "metadata": {
        "id": "qzQlZdFtkJ2y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model parameters\n",
        "input_dim = 10\n",
        "output_dim = 5"
      ],
      "metadata": {
        "id": "wZrQcYZNkKDh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy data for demonstration\n",
        "inputs = torch.randn(8, input_dim)\n",
        "targets = torch.randn(8, output_dim)"
      ],
      "metadata": {
        "id": "Aw3wu_ZpkfTa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# QLoRA ranks to evaluate\n",
        "qlora_ranks = [1, 2, 3, 4]"
      ],
      "metadata": {
        "id": "GWOl1BcwknRW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store all losses\n",
        "all_losses = {\"No QLoRA\": train_model(SimpleModel(input_dim, output_dim), inputs, targets)}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2BRaD-9kndy",
        "outputId": "17194c76-0e86-4a77-ee9b-2b7b8b639153"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.7408\n",
            "Epoch [20/100], Loss: 0.4374\n",
            "Epoch [30/100], Loss: 0.3043\n",
            "Epoch [40/100], Loss: 0.2410\n",
            "Epoch [50/100], Loss: 0.2009\n",
            "Epoch [60/100], Loss: 0.1714\n",
            "Epoch [70/100], Loss: 0.1487\n",
            "Epoch [80/100], Loss: 0.1308\n",
            "Epoch [90/100], Loss: 0.1156\n",
            "Epoch [100/100], Loss: 0.1023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train models with different QLoRA ranks\n",
        "for rank in qlora_ranks:\n",
        "    print(f\"Training with QLoRA rank {rank}...\")\n",
        "    model_qlora = SimpleModel(input_dim, output_dim, rank)\n",
        "    losses_qlora = train_model(model_qlora, inputs, targets)\n",
        "    all_losses[f\"QLoRA rank {rank}\"] = losses_qlora\n",
        "    model_qlora.print_weights()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlHvywf4kvtI",
        "outputId": "665e0268-1cb9-4489-cdb8-0f595b7d4ee7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with QLoRA rank 1...\n",
            "Epoch [10/100], Loss: 1.4289\n",
            "Epoch [20/100], Loss: 1.3887\n",
            "Epoch [30/100], Loss: 1.4202\n",
            "Epoch [40/100], Loss: 1.4721\n",
            "Epoch [50/100], Loss: 1.5063\n",
            "Epoch [60/100], Loss: 1.5068\n",
            "Epoch [70/100], Loss: 1.4924\n",
            "Epoch [80/100], Loss: 1.5402\n",
            "Epoch [90/100], Loss: 1.5960\n",
            "Epoch [100/100], Loss: 1.6197\n",
            "Full-rank weight matrix (W): \n",
            "tensor([[-0.0087, -0.1495, -0.1074, -0.0533,  0.0293,  0.0983,  0.2615,  0.0471,\n",
            "         -0.2149,  0.0226],\n",
            "        [-0.1343, -0.4277, -0.2102, -0.0246,  0.2071,  0.3349, -0.0436,  0.0741,\n",
            "          0.1004,  0.0463],\n",
            "        [-0.0469,  0.1522, -0.0928,  0.2001, -0.1729,  0.1245,  0.0027,  0.0351,\n",
            "         -0.1612,  0.1501],\n",
            "        [-0.2156,  0.0906, -0.1339,  0.0184,  0.0485,  0.2646,  0.1240, -0.1214,\n",
            "          0.0041, -0.0635],\n",
            "        [-0.0921, -0.2336,  0.0981,  0.2251,  0.1633,  0.2597,  0.4212,  0.0176,\n",
            "         -0.2575, -0.0151]])\n",
            "Low-rank weight matrix A: \n",
            "tensor([[-0.1087, -0.1654, -0.1439,  0.3111,  0.0326,  0.2459,  0.2305,  0.0520,\n",
            "         -0.2130,  0.0507]])\n",
            "Low-rank weight matrix B: \n",
            "tensor([[0.1738],\n",
            "        [0.1245],\n",
            "        [0.2280],\n",
            "        [0.1217],\n",
            "        [0.1241]])\n",
            "Training with QLoRA rank 2...\n",
            "Epoch [10/100], Loss: 1.7032\n",
            "Epoch [20/100], Loss: 1.5301\n",
            "Epoch [30/100], Loss: 1.3609\n",
            "Epoch [40/100], Loss: 1.3045\n",
            "Epoch [50/100], Loss: 1.3215\n",
            "Epoch [60/100], Loss: 1.4302\n",
            "Epoch [70/100], Loss: 1.4874\n",
            "Epoch [80/100], Loss: 1.5174\n",
            "Epoch [90/100], Loss: 1.5200\n",
            "Epoch [100/100], Loss: 1.5147\n",
            "Full-rank weight matrix (W): \n",
            "tensor([[ 0.0129,  0.2963, -0.2114,  0.1124,  0.0312,  0.2696, -0.1422,  0.0948,\n",
            "         -0.0303,  0.1728],\n",
            "        [ 0.1847, -0.2721,  0.0361, -0.0268, -0.0813,  0.0412,  0.3254, -0.0906,\n",
            "         -0.0799, -0.3371],\n",
            "        [ 0.2284, -0.1343, -0.1332, -0.1528,  0.1105,  0.1332,  0.1709,  0.0142,\n",
            "         -0.0755, -0.1736],\n",
            "        [-0.0544, -0.3251, -0.0634, -0.0081, -0.1427,  0.1548, -0.1171, -0.0894,\n",
            "         -0.0669, -0.0709],\n",
            "        [ 0.0970,  0.0047,  0.0087, -0.0402, -0.1094, -0.0358, -0.1424, -0.0515,\n",
            "         -0.0312, -0.0525]])\n",
            "Low-rank weight matrix A: \n",
            "tensor([[ 0.2988,  0.0158,  0.2011, -0.4843, -0.1515, -0.4855, -0.2725, -0.0547,\n",
            "          0.0972, -0.1147],\n",
            "        [ 0.3205, -0.0009,  0.2670, -0.5300, -0.2051, -0.5389, -0.3247, -0.0648,\n",
            "          0.0373, -0.1619]])\n",
            "Low-rank weight matrix B: \n",
            "tensor([[-0.2201, -0.2127],\n",
            "        [-0.1947, -0.2161],\n",
            "        [-0.1438, -0.1449],\n",
            "        [-0.2290, -0.2218],\n",
            "        [-0.2648, -0.2777]])\n",
            "Training with QLoRA rank 3...\n",
            "Epoch [10/100], Loss: 1.2219\n",
            "Epoch [20/100], Loss: 1.2386\n",
            "Epoch [30/100], Loss: 1.2705\n",
            "Epoch [40/100], Loss: 1.3057\n",
            "Epoch [50/100], Loss: 1.3016\n",
            "Epoch [60/100], Loss: 1.2969\n",
            "Epoch [70/100], Loss: 1.2988\n",
            "Epoch [80/100], Loss: 1.3028\n",
            "Epoch [90/100], Loss: 1.3076\n",
            "Epoch [100/100], Loss: 1.3151\n",
            "Full-rank weight matrix (W): \n",
            "tensor([[ 0.1623, -0.1187, -0.0570,  0.1709, -0.2406,  0.3565,  0.3622,  0.2142,\n",
            "          0.0931, -0.0270],\n",
            "        [-0.0379,  0.2873, -0.1008, -0.1313, -0.0066,  0.1798,  0.0477,  0.0434,\n",
            "         -0.4305,  0.0934],\n",
            "        [-0.1162, -0.1609, -0.0495,  0.1007, -0.0386,  0.1795,  0.2383, -0.1041,\n",
            "          0.0847,  0.0373],\n",
            "        [-0.2475, -0.0836, -0.1852,  0.0954,  0.1543,  0.0295,  0.1999,  0.0390,\n",
            "          0.1965, -0.1771],\n",
            "        [-0.0848, -0.1617,  0.0473,  0.2965, -0.0618,  0.1782,  0.3308, -0.1376,\n",
            "          0.0048, -0.2209]])\n",
            "Low-rank weight matrix A: \n",
            "tensor([[ 0.0578,  0.1081, -0.0657,  0.0636,  0.0831,  0.1278, -0.1125, -0.1268,\n",
            "         -0.0910, -0.1331],\n",
            "        [-0.0533, -0.0428,  0.1807, -0.1800, -0.0718, -0.1083,  0.0102,  0.0725,\n",
            "          0.1162,  0.0930],\n",
            "        [ 0.0703, -0.0101, -0.1101,  0.1837,  0.0751,  0.1216,  0.0497, -0.0959,\n",
            "         -0.1273, -0.0379]])\n",
            "Low-rank weight matrix B: \n",
            "tensor([[ 0.1050, -0.1135,  0.1694],\n",
            "        [ 0.0710, -0.0657,  0.0669],\n",
            "        [ 0.1493, -0.1958,  0.1789],\n",
            "        [ 0.0100, -0.0020,  0.0129],\n",
            "        [-0.1235, -0.1077,  0.0881]])\n",
            "Training with QLoRA rank 4...\n",
            "Epoch [10/100], Loss: 1.3405\n",
            "Epoch [20/100], Loss: 1.2264\n",
            "Epoch [30/100], Loss: 1.2211\n",
            "Epoch [40/100], Loss: 1.3417\n",
            "Epoch [50/100], Loss: 1.4605\n",
            "Epoch [60/100], Loss: 1.5527\n",
            "Epoch [70/100], Loss: 1.5199\n",
            "Epoch [80/100], Loss: 1.5320\n",
            "Epoch [90/100], Loss: 1.6158\n",
            "Epoch [100/100], Loss: 1.6129\n",
            "Full-rank weight matrix (W): \n",
            "tensor([[ 0.1777, -0.2073, -0.2096,  0.4266,  0.0471,  0.0666, -0.0063, -0.1201,\n",
            "         -0.1736,  0.3098],\n",
            "        [-0.0241, -0.1481, -0.2547, -0.0511,  0.1405,  0.2539, -0.0422, -0.0218,\n",
            "          0.0268,  0.0049],\n",
            "        [ 0.0839, -0.1461,  0.0176, -0.1377,  0.1366, -0.0535,  0.4014, -0.2179,\n",
            "         -0.1570, -0.1579],\n",
            "        [-0.1471,  0.0628, -0.3409, -0.3932,  0.1085,  0.0224, -0.0721,  0.0895,\n",
            "         -0.1925, -0.2624],\n",
            "        [ 0.0219, -0.2629,  0.0907, -0.1100,  0.1638,  0.0589,  0.0771,  0.1057,\n",
            "         -0.0795, -0.2951]])\n",
            "Low-rank weight matrix A: \n",
            "tensor([[-0.2940, -0.0942, -0.2398,  0.3455, -0.1589,  0.4162,  0.3287,  0.1572,\n",
            "         -0.0152,  0.0970],\n",
            "        [ 0.2839,  0.2019,  0.1899, -0.3527,  0.1450, -0.3428, -0.2788, -0.2097,\n",
            "          0.0951, -0.1270],\n",
            "        [-0.2744, -0.0828, -0.2315,  0.3213, -0.2046,  0.4344,  0.3385,  0.1654,\n",
            "         -0.0274,  0.0650],\n",
            "        [-0.0361, -0.0104,  0.1062,  0.0556,  0.1996, -0.3164, -0.0966,  0.0130,\n",
            "         -0.1187,  0.0785]])\n",
            "Low-rank weight matrix B: \n",
            "tensor([[ 0.1160, -0.0964,  0.1235, -0.0195],\n",
            "        [ 0.2336, -0.2418,  0.2277, -0.0012],\n",
            "        [ 0.1687, -0.1903,  0.1632, -0.0838],\n",
            "        [ 0.1085, -0.0429,  0.1435, -0.1653],\n",
            "        [ 0.2819, -0.2633,  0.2916, -0.1878]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the losses for all models\n",
        "plt.figure(figsize=(12, 8))\n",
        "for label, losses in all_losses.items():\n",
        "    plt.plot(losses, label=label)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss with and without QLoRA')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AtlKVg7okv4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example prediction\n",
        "test_inputs = torch.randn(2, input_dim)\n",
        "for rank in [\"No QLoRA\"] + [f\"QLoRA rank {r}\" for r in qlora_ranks]:\n",
        "    print(f\"Evaluating model with {rank}...\")\n",
        "    model = SimpleModel(input_dim, output_dim) if rank == \"No QLoRA\" else SimpleModel(input_dim, output_dim, int(rank.split()[-1]))\n",
        "    evaluate_model(model, test_inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ221ku9kwED",
        "outputId": "d2ad13f1-e08a-4937-c537-78759c3bbc67"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model with No QLoRA...\n",
            "Predictions: tensor([[ 0.2917,  0.1646, -0.2920, -1.1431, -0.6351],\n",
            "        [-0.6976,  0.0238, -0.4948, -0.2290, -0.8304]])\n",
            "Evaluating model with QLoRA rank 1...\n",
            "Predictions: tensor([[-0.3238,  1.1967,  0.1502,  0.2112,  0.2957],\n",
            "        [ 0.3848,  0.6570, -0.7321,  0.5209, -0.7650]])\n",
            "Evaluating model with QLoRA rank 2...\n",
            "Predictions: tensor([[ 0.0568,  0.4424,  0.0090, -0.1285,  0.7623],\n",
            "        [-0.5740, -0.2511,  0.7414,  0.5560,  0.1196]])\n",
            "Evaluating model with QLoRA rank 3...\n",
            "Predictions: tensor([[ 0.3698,  0.6944, -0.1524,  0.1299,  0.3783],\n",
            "        [-1.0727,  0.7198, -0.3839,  0.3500,  0.2371]])\n",
            "Evaluating model with QLoRA rank 4...\n",
            "Predictions: tensor([[-0.5002,  0.6161,  0.0497,  0.0795, -0.0795],\n",
            "        [ 0.6459,  0.8447,  0.2981, -0.5830,  0.2683]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E2XAjRVEknsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0oxFjis7Zri"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "arDrLQB4kd3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oe_jyBzZkeXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-MsDDBdCketH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ohDrNr-OkaKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N6JkH3aVkaSD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
