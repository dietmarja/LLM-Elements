{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWjKBSlRYiL6TSTfn0s9GP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dietmarja/LLM-Elements/blob/main/vector_db/pinecone_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query-Document Matching Using Pinecone  \n",
        "Given a query the code identifies similar pages of a document. To get the embeddings the match is based upon, the code harnesses Xenova's public domain model all-MiniLM-L6-v2 via the transformer library of Huggingface"
      ],
      "metadata": {
        "id": "W3YR5PxRiLvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip qU install transformers pinecone-client torch langchain PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5MqMyhNkvxP",
        "outputId": "d054c2a3-dab4-4a90-c9e2-2ad87aad1bf5"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: unknown command \"qU\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "from langchain import LLMChain\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import pinecone\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "KCNrOrsjbfWF"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Pinecone API Keys\n",
        "import os\n",
        "from google.colab import userdata\n",
        "pinecone_api_key = userdata.get('PINECONE_API_KEY')\n",
        "\n",
        "# Initialize Pinecone\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# Create a pincecone client\n",
        "pc = Pinecone(api_key=pinecone_api_key)\n",
        "\n",
        "# Create a pincecone client\n",
        "index_name = \"docs-quickstart-index\"\n",
        "\n",
        "# Check if the index exists and delete it if it does\n",
        "if index_name in pc.list_indexes().names():\n",
        "    pc.delete_index(index_name)\n",
        "    print(\"Existing index deleted\")\n",
        "\n",
        "# Create the index\n",
        "pc.create_index(\n",
        "    name=index_name,\n",
        "    dimension=384,\n",
        "    metric=\"cosine\",  # Available metrics: \"euclidean\"/\"manhattan\"/\"dotproduct\"/\"cosine\"\n",
        "    spec=ServerlessSpec(\n",
        "        cloud='aws',\n",
        "        region='us-east-1'\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Index created successfully\")\n",
        "\n",
        "# Connect pc to the index\n",
        "index = pc.Index(index_name)\n",
        "print(\"Connected to the index\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTgbyy0thHno",
        "outputId": "ff00ccef-3fc9-4c54-b841-b06a39a4959b"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existing index deleted\n",
            "Index created successfully\n",
            "Connected to the index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "import pinecone\n",
        "import torch\n",
        "\n",
        "# Load a all-MiniLM-L6-v2 model and corresponding tokenizer\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "# Define a simple embedding function using Hugging Face model\n",
        "def embed_text(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        embeddings = model(**inputs).last_hidden_state.mean(dim=1).squeeze().tolist()\n",
        "        # Ensure the embedding is a flat list of floats\n",
        "        if isinstance(embeddings[0], list):\n",
        "            embeddings = [item for sublist in embeddings for item in sublist]\n",
        "    if len(embeddings) != 384:\n",
        "        raise ValueError(f\"Embedding dimension mismatch: Expected 384, got {len(embeddings)}\")\n",
        "    return embeddings\n"
      ],
      "metadata": {
        "id": "B4mdYImGPT3a"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import pinecone\n",
        "import torch\n",
        "\n",
        "# Load a Hugging Face model and tokenizer\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Define a simple embedding function using Hugging Face model\n",
        "def embed_text(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        embeddings = model(**inputs).last_hidden_state.mean(dim=1).squeeze().tolist()\n",
        "        # Ensure the embedding is a flat list of floats\n",
        "        if isinstance(embeddings[0], list):\n",
        "            embeddings = [item for sublist in embeddings for item in sublist]\n",
        "    if len(embeddings) != 384:\n",
        "        raise ValueError(f\"Embedding dimension mismatch: Expected 384, got {len(embeddings)}\")\n",
        "    return embeddings\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    texts = []\n",
        "    with fitz.open(pdf_path) as doc:\n",
        "        for page_num, page in enumerate(doc):\n",
        "            text = page.get_text()\n",
        "            texts.append((page_num, text))\n",
        "    return texts\n",
        "\n",
        "# Function to store document in Pinecone\n",
        "def store_document(doc_id, text):\n",
        "    embedding = embed_text(text)\n",
        "    index.upsert(vectors=[(doc_id, embedding)])\n",
        "\n",
        "# Path to your PDF file\n",
        "pdf_path_1 = \"Attention_Paper.pdf\"\n",
        "pdf_path_2 = \"Lora_Paper.pdf\"\n",
        "\n",
        "# Extract text from PDF\n",
        "texts_1 = extract_text_from_pdf(pdf_path_1)\n",
        "texts_2 = extract_text_from_pdf(pdf_path_2)\n",
        "\n",
        "\n",
        "# Store each page as a separate vector in Pinecone\n",
        "for page_num, text in texts_1:    # 11 papges -> 11 vectors\n",
        "    store_document(f\"doc_1_page_{page_num}\", text)\n",
        "\n",
        "for page_num, text in texts_2:    # 26 papges -> 26 vectors\n",
        "    store_document(f\"doc_2_page_{page_num}\", text)\n"
      ],
      "metadata": {
        "id": "rTpBuf54UIdI"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import pinecone\n",
        "import torch\n",
        "\n",
        "# Load a Hugging Face model and tokenizer\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Define a simple embedding function using Hugging Face model\n",
        "def embed_text(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        embeddings = model(**inputs).last_hidden_state.mean(dim=1).squeeze().tolist()\n",
        "        # Ensure the embedding is a flat list of floats\n",
        "        if isinstance(embeddings[0], list):\n",
        "            embeddings = [item for sublist in embeddings for item in sublist]\n",
        "    if len(embeddings) != 384:\n",
        "        raise ValueError(f\"Embedding dimension mismatch: Expected 384, got {len(embeddings)}\")\n",
        "    return embeddings\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    texts = []\n",
        "    with fitz.open(pdf_path) as doc:\n",
        "        for page_num, page in enumerate(doc):\n",
        "            text = page.get_text()\n",
        "            texts.append((page_num, text))\n",
        "    return texts\n",
        "\n",
        "# Function to store document in Pinecone\n",
        "def store_document(doc_id, text):\n",
        "    embedding = embed_text(text)\n",
        "    index.upsert(vectors=[(doc_id, embedding)])\n",
        "\n",
        "\n",
        "# Function to fetch document embeddings from Pinecone\n",
        "def fetch_document_embedding(doc_id):\n",
        "    response = index.fetch(ids=[doc_id])\n",
        "    return response['vectors'][doc_id]['values']\n",
        "\n",
        "\n",
        "# Path to your PDF file\n",
        "pdf_path_1 = \"Attention_Paper.pdf\"\n",
        "pdf_path_2 = \"Lora_Paper.pdf\"\n",
        "\n",
        "# Extract text from PDF\n",
        "texts_1 = extract_text_from_pdf(pdf_path_1)\n",
        "texts_2 = extract_text_from_pdf(pdf_path_2)\n",
        "\n",
        "# Store each page as a separate document in Pinecone\n",
        "for page_num, text in texts_1:\n",
        "    store_document(f\"doc_1_page_{page_num}\", text)\n",
        "\n",
        "for page_num, text in texts_2:\n",
        "    store_document(f\"doc_2_page_{page_num}\", text)\n",
        "\n"
      ],
      "metadata": {
        "id": "t0_yOhM_f4C4"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch embeddings for a specific document and specific page\n",
        "doc_1_page_0_embedding = fetch_document_embedding(\"doc_1_page_0\")\n",
        "print(f\"Embedding for doc_1_page_0: {doc_1_page_0_embedding}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEbhozBamjl-",
        "outputId": "0c799ebf-10b4-472b-941d-4640c0f6c203"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding for doc_1_page_0: [-0.206853062, -0.223999053, 0.046813827, -0.0230643358, 0.0230432414, 0.071560055, -0.113816455, 0.0820734054, 0.179481059, -0.0645837858, 0.000451715663, -0.0589559264, -0.0238105282, 0.0359089114, -0.148885295, -0.0010653967, 0.00790819712, 0.158959687, -0.219155401, -0.0994952619, 0.145848, 0.0338694155, 0.0657733828, 0.0922754109, -0.0230305381, 0.00614938326, -0.0647270307, -0.0973173678, -0.166990086, -0.0636097863, 0.0896595865, 0.0611081757, -0.0194371808, 0.15370205, -0.0951550752, 0.122550845, -0.20127055, -0.0460345298, -0.054356724, -0.137010694, -0.016428465, 0.0209778473, -0.0663719326, 0.0116982833, 0.252382189, -0.0417954028, 0.0444649272, -0.0279237218, 0.0591082424, -0.00949567, -0.169647872, 0.0829335749, -0.111427121, 0.205787525, -0.100707196, -0.0886689276, -0.0747392923, 0.012416536, -0.0711478591, -0.0783602595, -0.144999057, -0.0780568421, -0.024562031, -0.157820642, 0.0198680833, -0.0756786317, 0.0277286582, 0.0444582179, -0.178535908, 0.0315816179, 0.0726046115, 0.102905393, -0.0543546155, 0.0336768702, -0.00792199932, 0.0987222418, 0.189427242, 0.0762763694, 0.11322248, -0.13334012, 0.0145513583, 0.0147677977, 0.161471128, -0.0425100811, 0.125390574, -0.0500735231, -0.0566081516, 0.166223243, 0.0884746462, 0.0649781078, -0.0412964076, -0.0964796394, 0.118448772, -0.0468441211, 0.130911767, 0.0631396696, 0.0559975132, 0.0129136071, -0.0853050575, 0.142387182, 0.0278618075, 0.110797338, 0.0541316941, -0.171836853, -0.0354568139, 0.0371615663, 0.146760747, 0.0325966105, 0.0951490179, -0.185571402, 0.0202314258, 0.129530683, 0.0629199147, -0.0804282874, 0.0673882514, -0.0486404449, -0.0240803696, -0.0845134705, 0.0997374579, 0.0772010088, -0.0655040443, 0.00992705394, -0.0854861438, 0.0563898608, -0.0320212692, -0.07912305, -0.0334645435, 1.18490756e-32, -8.37817788e-06, 0.132540166, 0.0424667373, -0.0829208493, 0.0608211458, -0.0308454446, -0.00946107507, 0.153648823, -0.143021584, -0.106234461, -0.13594982, 0.014988889, -0.1667009, 0.15865691, 0.00261620153, -0.0831055194, -0.00535751367, 0.0953863263, -0.0258254372, -0.0620156229, 0.00909549929, 0.0240934715, 0.0338680968, -0.0212924592, 0.0558354668, 0.0241737105, 0.0782923624, -0.176183239, -0.195840761, -0.00665845815, -0.212736055, 0.0144301048, 0.0264497325, -0.0489247404, 0.0346456096, -0.303115129, 0.00304269604, -0.101136386, 0.0295785554, -0.150260165, 0.0151258688, 0.152768701, -0.0541992188, 0.0455073714, -0.108451992, -0.0562271178, 0.0209958591, 0.0304884966, 0.127646521, -0.0407416075, 0.00872874074, -0.00728192553, -0.21752122, -0.195992634, 0.062697351, 0.167443, 0.118143111, 0.165100113, 0.0956896245, 0.221861959, -0.00620577857, 0.173377395, 0.0308521539, 0.190635294, 0.139689, -0.013099432, -0.134135991, 0.195186913, 0.0879368633, -0.0425767824, -0.110309139, 0.0233347081, 0.135024652, 0.0402439386, 0.146133795, 0.0017107483, 0.0646010935, -0.218458265, -0.130207658, 0.0194838084, 0.0176072, 0.0525731519, -0.123975456, -0.045476567, -0.110966764, 0.0293427669, 0.215955853, -0.0715076327, -0.00717320386, -0.0272019077, 0.0209952481, -0.0178603604, -0.0281559452, -0.00864995271, 0.0107270498, -6.51858858e-33, -0.0168411322, -0.00497886445, -0.121510699, 0.111304194, -0.0392718092, -0.0669495761, -0.027143145, 0.0869446099, -0.0480962321, -0.0109732524, -0.0709278733, -0.134339958, 0.155809969, 0.036172919, 0.0978153124, 0.00705505675, 0.0692003369, 0.00576290116, -0.0809158385, 0.102895617, -0.0342982337, 0.327165663, -0.172484189, -0.0694267, -0.181016088, 0.0578029752, -0.0626433864, 0.254929721, -0.00531250425, -0.0585565083, -0.0147895189, 0.0845800564, -0.128763661, 0.0376702212, -0.0785199255, 0.0943684876, 0.0923034102, -0.110341884, -0.0943612903, 0.0535318404, 0.189212084, -0.00342061557, 0.0107805477, -0.016756637, -0.0486688, -0.0551412776, -0.213928014, -0.0303573869, -0.0537620075, 0.103069067, -0.0249963813, 0.0648055822, -0.198232591, -0.00358578376, 0.0256579295, -0.144589394, -0.0930275917, -0.17307356, 0.0447856858, -0.168788463, -0.0486928076, -0.0894881338, 0.112614349, -0.154440105, 0.0999548286, 0.0669872537, 0.063804388, 0.0361795761, 0.210004687, -0.00283405744, -0.0271134879, -0.140608549, 0.16783008, 0.0355850384, -0.0198962968, -0.0411090553, 0.0278649032, -0.0634897798, -0.0667705387, -0.178451657, -0.129959494, -0.0805478543, -0.0795524567, 0.0352240056, 0.128718913, 0.0171681736, 0.165661037, 0.0646138936, 0.15771094, 0.0718611404, -0.00462157652, 0.0857524872, 0.0676487461, 0.199432507, -0.101267584, -9.86841826e-08, -0.0884469897, 0.00760256406, 0.0281325169, 0.0506021976, 0.0921505392, -0.25150916, -0.0291693453, 0.0401335, -0.0659398809, 0.070528, 0.128723353, 0.0302617569, -0.131053358, -0.00378157059, -0.0793486908, 0.193398744, 0.124790922, -0.014962865, 0.0603473298, -0.0344418213, 0.090009436, 0.190348268, 0.0209821649, 0.0183011889, 0.0168688446, -0.0443306118, -0.0906339213, 0.0672909468, 0.0465669781, -0.119553708, 0.058161281, 0.0820352212, 0.076838322, -0.139286116, 0.0433658957, -0.0209775139, 0.119987488, 0.119023725, 0.00735313, 0.0442656688, 0.163198858, 0.0805013, -0.0752078295, 0.0827894732, -0.00198358111, -0.106962353, -0.085451737, -0.179871723, 0.130549237, -0.0992353708, 0.160749495, -0.0569009632, 0.0420578122, 0.0491655022, 0.0561761856, 0.0902198926, 0.0562655106, -0.0986453369, 0.0222082883, 0.118194014, 0.095312655, 0.142167687, -0.110534832, 0.0428846665]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Pinecone query related to the attention paper\n",
        "query_text = \"Where can I find key aspects of the attention mechanisms\"\n",
        "query_embedding = embed_text(query_text)\n",
        "results = index.query(vector=query_embedding, top_k=3)\n",
        "\n",
        "print(\"Results:\")\n",
        "for match in results['matches']:\n",
        "    doc_id = match['id']\n",
        "    score = match['score']\n",
        "    print(f\"Document ID: {doc_id}, Score: {score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAaJDLgcgyM7",
        "outputId": "433b5b34-0de3-448b-ad0d-d3047dc5c410"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results:\n",
            "Document ID: doc_1_page_4, Score: 0.399698615\n",
            "Document ID: doc_1_page_3, Score: 0.383235663\n",
            "Document ID: doc_1_page_6, Score: 0.34568134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Pinecone query related to the lora paper\n",
        "query_text = \"Where can I find key aspects of low-rank adaptation\"\n",
        "query_embedding = embed_text(query_text)\n",
        "results = index.query(vector=query_embedding, top_k=3)\n",
        "\n",
        "print(\"Results:\")\n",
        "for match in results['matches']:\n",
        "    doc_id = match['id']\n",
        "    score = match['score']\n",
        "    print(f\"Document ID: {doc_id}, Score: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAJFWKFigfuQ",
        "outputId": "0e243b41-0f05-4440-c51f-5e67b704aca6"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results:\n",
            "Document ID: doc_2_page_23, Score: 0.488661855\n",
            "Document ID: doc_2_page_12, Score: 0.488027722\n",
            "Document ID: doc_2_page_1, Score: 0.447766632\n"
          ]
        }
      ]
    }
  ]
}